{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import modules and packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape == (1515, 7)\n",
      "All timestamps == 1515\n",
      "Featured selected: ['Open', 'High', 'Low', 'Close', 'Adj Close']\n"
     ]
    }
   ],
   "source": [
    "# Importing Training Set\n",
    "dataset_train = pd.read_csv('TCS.NS.csv')\n",
    "\n",
    "# Select features (columns) to be involved intro training and predictions\n",
    "cols = list(dataset_train)[1:6]\n",
    "\n",
    "# Extract dates (will be used in visualization)\n",
    "datelist_train = list(dataset_train['Date'])\n",
    "datelist_train = [dt.datetime.strptime(date, '%Y-%m-%d').date() for date in datelist_train]\n",
    "\n",
    "print('Training set shape == {}'.format(dataset_train.shape))\n",
    "print('All timestamps == {}'.format(len(datelist_train)))\n",
    "print('Featured selected: {}'.format(cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training set == (1515, 5).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1219.5     , 1219.5     , 1206.125   , 1208.199951, 1075.699341],\n",
       "       [1205.074951, 1207.      , 1183.025024, 1184.800049, 1054.865723],\n",
       "       [1192.5     , 1193.300049, 1170.5     , 1174.474976, 1045.672974],\n",
       "       ...,\n",
       "       [3786.      , 3835.      , 3748.      , 3817.800049, 3817.800049],\n",
       "       [3844.      , 3854.100098, 3806.      , 3813.100098, 3813.100098],\n",
       "       [3825.      , 3835.      , 3779.      , 3784.199951, 3784.199951]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train = dataset_train[cols].astype(str)\n",
    "for i in cols:\n",
    "    for j in range(0, len(dataset_train)):\n",
    "        dataset_train[i][j] = dataset_train[i][j].replace(',', '')\n",
    "\n",
    "dataset_train = dataset_train.astype(float)\n",
    "\n",
    "# Using multiple features (predictors)\n",
    "training_set = dataset_train.values\n",
    "\n",
    "print('Shape of training set == {}.'.format(training_set.shape))\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.02533526],\n",
       "       [-1.04349259],\n",
       "       [-1.05932114],\n",
       "       ...,\n",
       "       [ 2.20521124],\n",
       "       [ 2.27821794],\n",
       "       [ 2.25430195]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "training_set_scaled = sc.fit_transform(training_set)\n",
    "\n",
    "sc_predict = StandardScaler()\n",
    "sc_predict.fit_transform(training_set[:, 0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape == (1366, 90, 4).\n",
      "y_train shape == (1366, 1).\n"
     ]
    }
   ],
   "source": [
    "# Creating a data structure with 90 timestamps and 1 output\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "n_future = 60   # Number of days we want top predict into the future\n",
    "n_past = 90     # Number of past days we want to use to predict the future\n",
    "\n",
    "for i in range(n_past, len(training_set_scaled) - n_future +1):\n",
    "    X_train.append(training_set_scaled[i - n_past:i, 0:dataset_train.shape[1] - 1])\n",
    "    y_train.append(training_set_scaled[i + n_future - 1:i + n_future, 0])\n",
    "\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "\n",
    "print('X_train shape == {}.'.format(X_train.shape))\n",
    "print('y_train shape == {}.'.format(y_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>PART 2. Create a model. Training</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step #3. Building the LSTM based Neural Network</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries and packages from Keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initializing the Neural Network based on LSTM\n",
    "model = Sequential()\n",
    "\n",
    "# Adding 1st LSTM layer\n",
    "model.add(LSTM(units=64, return_sequences=True, input_shape=(n_past, dataset_train.shape[1]-1)))\n",
    "\n",
    "# Adding 2nd LSTM layer\n",
    "model.add(LSTM(units=10, return_sequences=False))\n",
    "\n",
    "# Adding Dropout\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(units=1, activation='linear'))\n",
    "\n",
    "# Compiling the Neural Network\n",
    "model.compile(optimizer = Adam(learning_rate=0.01), loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step #4. Start training</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "5/5 [==============================] - 10s 1s/step - loss: 0.2091 - val_loss: 0.5960\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.59595, saving model to weights.h5\n",
      "Epoch 2/30\n",
      "5/5 [==============================] - 1s 291ms/step - loss: 0.1063 - val_loss: 0.6426\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.59595\n",
      "Epoch 3/30\n",
      "5/5 [==============================] - 1s 298ms/step - loss: 0.0828 - val_loss: 0.2133\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.59595 to 0.21331, saving model to weights.h5\n",
      "Epoch 4/30\n",
      "5/5 [==============================] - 2s 315ms/step - loss: 0.0732 - val_loss: 0.2234\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.21331\n",
      "Epoch 5/30\n",
      "5/5 [==============================] - 2s 329ms/step - loss: 0.0681 - val_loss: 0.0794\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.21331 to 0.07936, saving model to weights.h5\n",
      "Epoch 6/30\n",
      "5/5 [==============================] - 2s 339ms/step - loss: 0.0663 - val_loss: 0.1682\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.07936\n",
      "Epoch 7/30\n",
      "5/5 [==============================] - 1s 292ms/step - loss: 0.0646 - val_loss: 0.1351\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.07936\n",
      "Epoch 8/30\n",
      "5/5 [==============================] - 2s 295ms/step - loss: 0.0602 - val_loss: 0.0998\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.07936\n",
      "Epoch 9/30\n",
      "5/5 [==============================] - 1s 289ms/step - loss: 0.0612 - val_loss: 0.0978\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.07936\n",
      "Epoch 10/30\n",
      "5/5 [==============================] - 2s 306ms/step - loss: 0.0618 - val_loss: 0.1316\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.07936\n",
      "Epoch 11/30\n",
      "5/5 [==============================] - 2s 303ms/step - loss: 0.0572 - val_loss: 0.1529\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.07936\n",
      "Epoch 12/30\n",
      "5/5 [==============================] - 2s 317ms/step - loss: 0.0623 - val_loss: 0.1370\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.07936\n",
      "Epoch 13/30\n",
      "5/5 [==============================] - 2s 316ms/step - loss: 0.0649 - val_loss: 0.1513\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.07936\n",
      "Epoch 14/30\n",
      "5/5 [==============================] - 2s 311ms/step - loss: 0.0596 - val_loss: 0.2456\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.07936\n",
      "Epoch 15/30\n",
      "5/5 [==============================] - 2s 305ms/step - loss: 0.0631 - val_loss: 0.2014\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.07936\n",
      "Epoch 00015: early stopping\n",
      "Wall time: 31.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "es = EarlyStopping(monitor='val_loss', min_delta=1e-10, patience=10, verbose=1)\n",
    "rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=1)\n",
    "mcp = ModelCheckpoint(filepath='weights.h5', monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "tb = TensorBoard('logs')\n",
    "\n",
    "history = model.fit(X_train, y_train, shuffle=True, epochs=30, callbacks=[es, rlr, mcp, tb], validation_split=0.2, verbose=1, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypertuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -q -U keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt\n",
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "    model = Sequential()\n",
    "    hp_units = hp.Int('units', min_value=32, max_value=512, step=32)\n",
    "    model.add(LSTM(units=64, return_sequences=True, input_shape=(n_past, dataset_train.shape[1]-1)))\n",
    "    model.add(LSTM(units=10, return_sequences=False))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(units=1, activation='linear'))\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "    model.compile(optimizer = Adam(learning_rate=hp_learning_rate), loss='mean_squared_error',metrics=tensorflow.keras.metrics.RootMeanSquaredError())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner1 = kt.Hyperband(hypermodel=model_builder,\n",
    "                     objective='val_loss',\n",
    "                     max_epochs=100,\n",
    "                     factor=3,\n",
    "                     hyperband_iterations=10,\n",
    "                     directory='abc',\n",
    "                     overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_early = tensorflow.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 16 Complete [00h 00m 20s]\n",
      "val_loss: 0.10821513831615448\n",
      "\n",
      "Best val_loss So Far: 0.07485309988260269\n",
      "Total elapsed time: 00h 18m 34s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "tuner1.search(X_train,y_train, epochs=5,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 1\n",
      "units (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 32, 'sampling': None}\n"
     ]
    }
   ],
   "source": [
    "tuner1.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "best=tuner1.get_best_hyperparameters(num_trials=3)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best.get('units')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'learning_rate does not exist.'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-13f1a3d21fa9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'learning_rate'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras_tuner\\engine\\hyperparameters.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    739\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{} is currently inactive.\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    740\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 741\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{} does not exist.\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    742\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    743\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'learning_rate does not exist.'"
     ]
    }
   ],
   "source": [
    "best.get('learning_rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
      "layer is 96 and the optimal learning rate for the optimizer\n",
      "is .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the optimal hyperparameters\n",
    "best_hps=tuner1.get_best_hyperparameters(num_trials=10)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
    "layer is {best_hps.get('units')} and the optimal learning rate for the optimizer\n",
    "is .\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "Notes:<br>\n",
    "<ul>\n",
    "<li><b>EarlyStopping</b> - Stop training when a monitored metric has stopped improving.</li>\n",
    "<li><code>monitor</code> - quantity to be monitored.</li>\n",
    "<li><code>min_delta</code> - minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than <code>min_delta</code>, will count as no improvement.</li>\n",
    "<li><code>patience</code> - number of epochs with no improvement after which training will be stopped.</li>\n",
    "</ul>\n",
    "\n",
    "<ul>\n",
    "<li><b>ReduceLROnPlateau</b> - Reduce learning rate when a metric has stopped improving.</li>\n",
    "<li><code>factor</code> - factor by which the learning rate will be reduced. <code>new_lr = lr * factor</code>.</li>\n",
    "</ul>\n",
    "</p>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<p>\n",
    "The last date for our training set is <code>30-Dec-2016</code>.<br>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "We will perform predictions for the next <b>20</b> days, since <b>2017-01-01</b> to <b>2017-01-20</b>.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>PART 3. Make future predictions</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate list of sequence of days for predictions\n",
    "datelist_future = pd.date_range(datelist_train[-1], periods=n_future, freq='1d').tolist()\n",
    "\n",
    "'''\n",
    "Remeber, we have datelist_train from begining.\n",
    "'''\n",
    "\n",
    "# Convert Pandas Timestamp to Datetime object (for transformation) --> FUTURE\n",
    "datelist_future_ = []\n",
    "for this_timestamp in datelist_future:\n",
    "    datelist_future_.append(this_timestamp.date())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step #5. Make predictions for future dates</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Perform predictions\n",
    "predictions_future = model.predict(X_train[-n_future:])\n",
    "\n",
    "predictions_train = model.predict(X_train[n_past:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Inverse the predictions to original measurements\n",
    "\n",
    "# ---> Special function: convert <datetime.date> to <Timestamp>\n",
    "def datetime_to_timestamp(x):\n",
    "    '''\n",
    "        x : a given datetime value (datetime.date)\n",
    "    '''\n",
    "    return datetime.strptime(x.strftime('%Y%m%d'), '%Y%m%d')\n",
    "\n",
    "\n",
    "y_pred_future = sc_predict.inverse_transform(predictions_future)\n",
    "y_pred_train = sc_predict.inverse_transform(predictions_train)\n",
    "\n",
    "PREDICTIONS_FUTURE = pd.DataFrame(y_pred_future, columns=['Open']).set_index(pd.Series(datelist_future))\n",
    "PREDICTION_TRAIN = pd.DataFrame(y_pred_train, columns=['Open']).set_index(pd.Series(datelist_train[2 * n_past + n_future -1:]))\n",
    "\n",
    "# Convert <datetime.date> to <Timestamp> for PREDCITION_TRAIN\n",
    "PREDICTION_TRAIN.index = PREDICTION_TRAIN.index.to_series().apply(datetime_to_timestamp)\n",
    "\n",
    "PREDICTION_TRAIN.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTION_TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTIONS_FUTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train=pd.DataFrame(dataset_train,columns=cols)\n",
    "dataset_train.index=datelist_train\n",
    "dataset_train.index=pd.to_datetime(dataset_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train[START_DATE_FOR_PLOTTING:]['Open']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step #6. Visualize the Predictions</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set plot size \n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 14, 5\n",
    "\n",
    "# Plot parameters\n",
    "START_DATE_FOR_PLOTTING = '2016-12-22'\n",
    "\n",
    "plt.plot(PREDICTIONS_FUTURE['Open'], color='r', label='Predicted Stock Price')\n",
    "plt.plot(PREDICTION_TRAIN.loc[:]['Open'], color='orange', label='Training predictions')\n",
    "plt.plot(dataset_train.loc[:].index, dataset_train.loc[:]['Open'], color='b', label='Actual Stock Price')\n",
    "\n",
    "plt.axvline(x = min(PREDICTIONS_FUTURE.index), color='green', linewidth=2, linestyle='--')\n",
    "\n",
    "plt.grid(which='major', color='#cccccc', alpha=0.5)\n",
    "\n",
    "plt.legend(shadow=True)\n",
    "plt.title('Predcitions and Acutal Stock Prices', family='Arial', fontsize=12)\n",
    "plt.xlabel('Timeline', family='Arial', fontsize=10)\n",
    "plt.ylabel('Stock Price Value', family='Arial', fontsize=10)\n",
    "plt.xticks(rotation=45, fontsize=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "MSE = np.sqrt(mean_squared_error(dataset_train[START_DATE_FOR_PLOTTING:]['Open'],PREDICTION_TRAIN))\n",
    "print(MSE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
